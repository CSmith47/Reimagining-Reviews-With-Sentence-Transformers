{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d851d805-1485-4d0e-84d8-affa25a55249",
   "metadata": {},
   "source": [
    "# Reimagining Reviews With Sentence Transformers\n",
    "## Author: Christian Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00280b1-3b82-499e-9e33-356cd4abca92",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850e025-0629-4d1e-8c5d-a754201314cc",
   "metadata": {},
   "source": [
    "A business’s image is central to its ability to operate. No client will accept a business with an inferior reputation as it is within everyone’s best interest to seek out the best service possible for the task at hand. Although big billboards, word-of-mouth marketing, first impressions, and endorsements have all been critical schemes to a company’s continued success, the digital age has brought about another avenue, one that far exceeds the others in importance, the online review. Before the era of the internet, it was often difficult to obtain information about a business before simply trying it out. Advertisements were biased, only represented the larger chains, and did not speak on the individual level. With the rise of the internet, companies, such as Yelp, began to offer a centralized platform where the people could relay their experiences with any business from franchised chains all the way down to the mom and pop shops. Online reviews have become the new normal due to their ease of access for reader’s looking for a specific service, their simplicity with each reviewer giving each review typically a range of one to five stars which gets averaged for the reader to quickly assess a business’s performance, and due to alleviation of bias when compared to business advertisements due to each review mostly being tied to a unique, unfiltered, genuine experience of a business’s services.\n",
    "\n",
    "As the popularity of online reviews began to grow, their influence has become difficult for companies to ignore, particularly in big cities where each service offered by a business is not unique, and thus the existence of the service itself is not a compelling reason for a customer to go. The growth of GPS for traversal has further compounded this issue by making it easier than ever for potential customers to choose the businesses with the best reviews, their decision often determined by a single number on a website like Yelp. Now that businesses are aware of the significance of reviews, how can they improve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8442c44-86de-40b0-98ad-6fd66cfa8fd5",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea83800-aec7-4231-818d-9f095e6df4fa",
   "metadata": {},
   "source": [
    "Due to the ease at which a review can be made on websites like Yelp, they can often quickly build up to an overwhelming size, leaving most business owners puzzled as to what they should improve about their company first. However, through the usage of machine learning, it is possible to identify which topics are the predominant concerns of customers. Within this article, I will explain how we can leverage machine learning to pinpoint topics of interest across thousands of reviews that a business could focus on to drive better reviews for their business to rely less on the luck of the individual customer review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8dc3e-2be8-4d0e-81a4-b99bce3398c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ed33a-34df-47f1-93cf-9213e3b66e8e",
   "metadata": {},
   "source": [
    "Before we begin, there are some important python libraries and two important datasets that will be used within the project. There will also be additional python libraries used later on when creating the Bertopic language analysis models, however those will be imported later in this notebook when they are required to be used. Both of these datasets are modifications of the official Yelp review dataset provided by Yelp on their website: [Yelp Open Dataset](https://www.yelp.com/dataset). Details on how I modified the \"business\" dataset will be available on the other attached file to this GitHub named: \"Business Attributes Dataset Modification Details\".\n",
    "\n",
    "reduced_business Dataset:  \n",
    "- <b>business_id</b>: A unique ID given to each business registered with Yelp  \n",
    "- <b>name</b>: The official name of the business  \n",
    "- <b>RestaurantsPriceRange2</b>: A discrete range of one to four where a four means the business is rated as one of the most expensive per person and a one means the business is rated as one of the cheapest per person. This metric is determined by a survey given to Yelp reviewers when checking in where 1 = under $\\$$10, 2 = $\\$$11-30, 3 = $\\$$31-60, and 4 = $\\$$61+ per person attending. While this column is called \"RestaurantsPriceRange2\", it also applies to some businesses that are not restaurants (i.e. Target)\n",
    "- <b>GoodForKids</b>: Either a 0 or 1 where 1 means the business is a good place for kids as determined by a survey\n",
    "- <b>NoiseLevel</b>: A categorical range from \"quiet\" to \"average\" to \"loud\" to \"very_loud\" in ascending levels of average noise at the business as voted on by a survey\n",
    "- <b>GoodForDancing</b>: Either a 0 or 1 where 1 means the business is a good place to dance as determined by a survey\n",
    "- <b>RestaurantsAttire</b>: A categorical range from \"casual\" to \"semi-formal\" to \"formal\" in ascending levels of average dress expectation at the business as voted on by a survey\n",
    "- <b>BikeParking</b>: Either a 0 or 1 where 1 means the business is has space for bike parking\n",
    "- <b>WheelchairAccessible</b>: Either a 0 or 1 where 1 means the business has wheelchair accessibility features\n",
    "- <b>categories</b>: A list that contains all of the categories the business is associated with  \n",
    "  \n",
    "reviews Dataset:  \n",
    "- <b>review_id</b>: A unique ID given to each review created on Yelp\n",
    "- <b>business_id</b>: The ID of the business the review is associated with\n",
    "- <b>stars</b>: The number of stars the reviewer gave the review\n",
    "- <b>text</b>: A string that contains the entire review as written by the reviewer\n",
    "- <b>date</b>: A datetime object that contains the year, month, day, and exact time at which the review was posted to Yelp's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303049b-5f2f-4e06-b338-9922c9e88cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Note: The OS library is required to remove some warnings with tensorflow that do not impact performance\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# Import reduced_business dataframe\n",
    "reduced_business = pd.read_csv('reduced_business.csv')\n",
    "\n",
    "# Import reviews dataframe\n",
    "# Note: Because the reviews dataframe is so large, it is best to import it in batches using the chunksize argument.\n",
    "# Note: Language analysis models are very resource intensive when used on large datasets. Due to this, I have decided to only use the first\n",
    "#       1,000,000 rows of reviews to maintain decent performance. The number of rows that can be used for model training largely depends on the\n",
    "#       resources your device has available, and the sample size desired.\n",
    "\n",
    "chunk_size = 1000  # Adjust the chunk size based on your file size\n",
    "chunks = pd.read_json('yelp_academic_dataset_review.json', lines=True, chunksize=chunk_size, nrows = 1000000) # use nrows to adjust sample size\n",
    "\n",
    "# Concatenate the chunks into a single DataFrame\n",
    "reviews = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c69ada-3a66-458c-9a60-65130cd6ce05",
   "metadata": {},
   "source": [
    "## What are Sentence Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11768cf5-9895-41a9-9caa-3089e50ea041",
   "metadata": {},
   "source": [
    "Before beginning to train our own machine learning models, we must transform the millions of reviews into usable data for the models. Because Natural Language Processing (NLP) models take vectors of integer values as input, we will need a transformer to convert the string of text into these integer vectors. For this task, we have sentence transformers, a type of pre-trained natural language processing model. The goal of sentence transformers is to ultimately convert sentences, or other short texts, into “embeddings”, fixed-dimensional vectors, which can then be used as input by other language processing models. The significance of sentence transformers is that they operate on the entire sentence rather than the individual words that make them up. This allows the embeddings to capture the semantics of each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c96157-2312-41d0-b686-2724d2cfa8c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What are the Use Cases for Sentence Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4add69-bb8f-42d1-a25d-636b485c7311",
   "metadata": {},
   "source": [
    "To illustrate the capability of sentence transformers, consider a scenario in which the stars given by the users were controversially hidden from their respective company, but still visible to customers. In this scenario, it would be crucial for business owners to be able to predict the number of stars they were given from the text of the review in order to assess the reputation of their business from the perspective of new, potential customers. Although the goal is simple, to predict the number of stars a review received given its text, the process to complete this is rather complex. This is largely due to the fact that humans express language in inconsistent ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e9d3d-9b30-4623-9474-4e78f1866473",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using VADER to Visualize the Complexity of Language Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f712d6-96a1-4e90-a474-2496000e5a9e",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based lexicon approach to generating vectors to encapsulate text. VADER works by applying several rules that judge sentence semantics against a large lexicon of words. VADER is a finer tuned count vectorizer model, as although it simply counts which words are present in each sentence, it assigns unique weights based on other factors. For example, the sentence, \"the food was TERRIBLE!!\" would be deemed by VADER to be more negative than \"the food was terrible\" due to the former's use of capitals letters and punctuation. Each text is then assigned a compound sentiment score that ranges from -1 to 1 where 1 is extremely positive and -1 is extremely negative. When visualizing the sentiment score across all reviews split based on the number of stars the review was given, we begin to see why predicting human language becomes so difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147aa10-38fb-4924-9040-52692fb18229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) start by initializing VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_compound(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# 2) create a new column in the model_reviews dataframe that displays each review's respective compound sentiment score\n",
    "model_reviews['compound'] = model_reviews['text'].apply(vader_compound)\n",
    "\n",
    "# 3) visualize the differences in sentiment between each star level\n",
    "fig, ((ax1, ax2, ax3, ax4, ax5)) = plt.subplots(1, 5, figsize = (15, 4))\n",
    "ax1.set_title('1 star')\n",
    "ax2.set_title('2 star')\n",
    "ax3.set_title('3 star')\n",
    "ax4.set_title('4 star')\n",
    "ax5.set_title('5 star')\n",
    "model_reviews[model_reviews['stars'] == 1]['compound'].plot(kind='box', ax=ax1)\n",
    "model_reviews[model_reviews['stars'] == 2]['compound'].plot(kind='box', ax=ax2) \n",
    "model_reviews[model_reviews['stars'] == 3]['compound'].plot(kind='box', ax=ax3)\n",
    "model_reviews[model_reviews['stars'] == 4]['compound'].plot(kind='box', ax=ax4)\n",
    "model_reviews[model_reviews['stars'] == 5]['compound'].plot(kind='box', ax=ax5)\n",
    "plt.savefig('vader_box.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80917b89-bfe9-4569-a29a-30cda7ed7ddf",
   "metadata": {},
   "source": [
    "When looking at the boxplots, VADER confirms that the median sentiment does increase as the star assigned to the review increases. This is expected behavior and how review stars are intended to function. However, what is not an expected result is an outlier 5 star review having a compound score that is close to -1, as negative as possible, or a 1 star review having a compound score of 1, as positive as possible. What is going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c0dad-0bf7-426e-be18-7e21e1ac398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this code block to find 1 star reviews that have an unusually high compound score\n",
    "low_mask = (model_reviews['stars'] == 1) & (model_reviews['compound'] > 0.9)\n",
    "low_positive = model_reviews.loc[low_mask].reset_index()\n",
    "low_positive['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe8860-adbb-4f50-84c8-3cf50b879a2e",
   "metadata": {},
   "source": [
    "When looking at 1 star reviews that have an unusually high compound score, we get returned reviews such as this one:\n",
    "\n",
    "\"As I ate at this restaurant I felt as if I was in a broadway production, a comedy, where everyone was doing their best to do the absolute opposite of good service. It was actually really entertaining, eventually. Toast was disgusting. Other food was good. Grits were really good.\"\n",
    "\n",
    "Here, we can see that the reviewer did indeed have a negative experience, but he used a comedic analogy to relay his terrible experience. Because of the words related to comedy and the reviewer’s compliments of some of the food, VADER assigned this review a high compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c92e6e-81ed-46f7-8fd2-2178de9bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this code block to find 5 star reviews that have an unusually low compound score\n",
    "high_mask = (model_reviews['stars'] == 5) & (model_reviews['compound'] < -0.9)\n",
    "high_negative = model_reviews.loc[high_mask].reset_index()\n",
    "high_negative['text'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588441c-d847-49ef-b039-50b8debd9e80",
   "metadata": {},
   "source": [
    "When looking at 5 star reviews that have an unusually low compound score, we get returned reviews such as this one:\n",
    "\n",
    "\"My daughter hit a re-tread tire on the interstate going 60mph. The tire shrapnel attacked the undercarriage of her car and snapped the exhaust pipe from the engine. We towed her car to Quick Auto as they seemed to be the closest exhaust specialist near us. Uncertain and slightly fearful of the extent of the damage, we literally received a call right when we walked back in the door saying the car was fixed and ready for pick up! The cost was dreamily affordable. Seriously contemplating moving all of our service needs for all of our vehicles to Quick Auto.\"\n",
    "\n",
    "Here, we can see that the reviewer spent most of the review describing the dreadful experience the reviewer had before visiting and receiving service from the business. This recap of the dreadful reasons for why the reviewer visited the business greatly overshadows the extremely positive sentiment that was included at the end, hence earning a low compound score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc0033-6f98-47b1-a41f-a424493e15b8",
   "metadata": {},
   "source": [
    "Due to the inconsistencies of how humans apply sentiment, context is crucial when analyzing languages using models. As shown by VADER, even reviews that are 5 stars can be mostly negative while 1 star reviews can sound like rave endorsements when the entire context is removed. For this reason, sentence transformers are pivotal to processing any form of language. Although imperfect, they continue to improve as they receive more data to become better at splitting topics from each other and considering even more context. The better question now is how can we leverage sentence transformers to drive better reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833909ea-25f9-4a79-8ee6-0eeaa435a0aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What is the Result of this Sentiment Inconsistency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be08ba-c0a7-4ed8-9147-a20e56d87b68",
   "metadata": {},
   "source": [
    "What is the end result of this inconsistent application of sentiment in reviews? Machine learning models have a difficult time predicting the number of stars a review obtains just from its text. We can run a logistic regression model to see the results of inputting inconsistent data into a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a3f42-d505-4282-97ad-a95916ce55c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Import Necessary Libraries/Dataset for the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592c989-f6ff-40aa-87aa-a58623b46b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b677c-f29a-4fb0-a433-3884929ec570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reviews = pd.DataFrame(reviews[['text', 'stars']].sample(100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d6dd8-7178-48cb-9a26-d6a6955c4098",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Remove All Punctuation and Stopwords From the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4a5f5-54af-4840-92e1-7b3803958d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = ''.join([char for char in mess if char not in string.punctuation])\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    #nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ebd2f-d46d-4cab-8094-fc0a601a6cee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Train and Fit the Data onto the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523fa35-22f5-4174-a302-377a9cea3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, Y_train, y_test = train_test_split(model_reviews['text'], model_reviews['stars'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455882e3-fa41-40ba-b55c-09658e3ec747",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression(max_iter = 1000)),  # train on TF-IDF vectors w/ Logistic Regression classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d73a9-fe3c-4c7b-911c-e39a641dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d057af8-f089-4790-8a2c-417894b639a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Analyze Accuracy of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d53b29-ef88-4420-bbc4-f2142ec8384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb9591-07c8-478a-b357-9421f2ec21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:\\n')\n",
    "print(confusion_matrix(predictions, y_test))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f38b4-e878-482a-9ec7-41cb071b1f49",
   "metadata": {},
   "source": [
    "Although the logistic regression model is able to predict the number of stars at each level, it is clear that is tends to favor placing each review at one of the two extreme values, at one or five stars. With an overall poor performance, the logistic regression model does not perform well when predicting the number of stars based solely on their frequency due to the inconsistency of human language.\n",
    "\n",
    "Due to the variations in how humans apply sentiment, context is crucial when analyzing languages using models. As shown by VADER, even reviews that are 5 stars can be mostly negative while 1 star reviews can sound like rave endorsements when the entire context is removed. For this reason, sentence transformers are pivotal to processing any form of language. Although imperfect, they continue to improve as they receive more data to become better at splitting topics from each other and considering even more context. The better question now is how can we leverage sentence transformers to drive better reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbf5d-9d78-4a92-9789-ac4724a1d165",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Leveraging Sentence Transformers to Drive Better Reviews for Businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e21e0d-1edb-41d9-b23c-ffd6f1e1f83a",
   "metadata": {},
   "source": [
    "When a business wants to increase their average star rating on websites like Yelp, a sensible place to start would be to find out what the reviewers themselves consider to be “exceptional service”. This task is not a simple one as each reviewer is not only unique, but also inconsistent in the way they tend to rate services. One of the best options, however, for pinpointing what reviewers deem important are the usage of machine learning models that utilize the strengths of sentence transformers to cluster together reviews into topics based on similar semantic meanings.\n",
    "\n",
    "For this task of uncovering which topics are significant in determining the outcome of customer reviews, we will be employing the use of the BERTopic model. BERTopic is a pre-trained language model that excels in grouping text into various topics that are then ranked by size, which is determined by the number of documents that are assigned to each topic. By narrowing down the reviews to apply to specific attributes of businesses, we will investigate how BERTopic can be utilized in different ways by all kinds of companies to focus on the topics that matter to reviewers the most. As a demonstration of its utility, we will put into practice how the BERTopic model simplifies the overwhelming number of reviews from relatively expensive businesses, Chinese restaurants, medical businesses, and sports and fitness businesses, and condenses them into easily identifiable topics that can shine a light onto overall review improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bbee6-a768-4332-ab01-2c753f107e43",
   "metadata": {},
   "source": [
    "### How Can We Use Bertopic to Analyze Reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25705537-4416-42ef-9af5-d350425355c1",
   "metadata": {},
   "source": [
    "As an introduction to Bertopic, we will see how we can analyze reviews using a subset of reviews associated with relatively expensive businesses (with a price range rating of 3 or 4). In order to train the Bertopic model, we will first need to create the subset of reviews for expensive businesses that we will be using for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a29e46-2497-49c7-81a5-d6514199e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create a subset of the reduced_business dataframe to only include businesses where their price range is a 3 or 4\n",
    "expensive = reduced_business[(reduced_business['RestaurantsPriceRange2'] == 3) | (reduced_business['RestaurantsPriceRange2'] == 4)]\n",
    "\n",
    "# 2) perform a left merge based on each business' unique ID to only keep expensive businesses while also associating each review with their respective business\n",
    "expensive_reviews = pd.merge(reviews, expensive, how = 'left', on = 'business_id')\n",
    "\n",
    "# 3) to perform any categorical analysis in the future, we will drop the few rows in which category information is missing\n",
    "expensive_reviews = expensive_reviews.dropna(subset=['categories'])\n",
    "\n",
    "# 4) reset the index of the new dataframe (will be necessary if a class-based analysis is performed)\n",
    "expensive_reviews = expensive_reviews.reset_index().drop('index', axis = 1)\n",
    "expensive_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1634ea-bb29-47c4-99f8-96167747355e",
   "metadata": {},
   "source": [
    "Next, we will import all of the required libraries necessary to run the Bertopic model. Additionally, we will set up the Bertopic model with its necessary parameters.\n",
    "\n",
    "<b>Note:</b> In order to utilize the GPU acceleration capability of Bertopic (within UMAP and HDBSCAN), Bertopic will need to be run within a WSL environment if on Windows. This is because the cuml library requires a linux-based environment to run. In order to create an environment suitable for GPU hardware acceleration:\n",
    "1. Ensure your device has a Nvidia GPU and the latest video driver update\n",
    "2. Download WSL onto your cmd line (i.e. Ubuntu 22.04)\n",
    "3. Install the Rapids 24.02 environment into WSL using this [Rapids install link](https://docs.rapids.ai/install).\n",
    "4. ```conda activate rapids-24.02``` environment and install Jupyter Notebook (if necessary)\n",
    "5. Run ```jupyter-notebook``` on the command line and navigate back to this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09b195-2388-4a0b-842e-53a51a8c2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.manifold import UMAP\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from plotly.offline import plot\n",
    "\n",
    "umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "hdbscan_model = HDBSCAN(min_samples=1, gen_min_span_tree=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df=10)\n",
    "representation_model = KeyBERTInspired() # This representation model can make it more obvious which topics are important -> condenses topics into key words that are similar to what are in the analyzed sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef0112-930d-49c2-b874-781284057d3d",
   "metadata": {},
   "source": [
    "Now, we will train the model on the review text data derived from the subset of expensive businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ceb5e-a0d5-40b8-9396-d6c9a656b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_reviews_to_analyze = expensive_reviews['text'].tolist()\n",
    "\n",
    "expensive_topic_model = BERTopic(vectorizer_model = vectorizer_model, umap_model = umap_model,\n",
    "                                 hdbscan_model = hdbscan_model, representation_model = representation_model)\n",
    "\n",
    "expensive_topics, expensive_probs = expensive_topic_model.fit_transform(exp_reviews_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b3d57-d191-41d7-969e-32054bd64ab4",
   "metadata": {},
   "source": [
    "After we complete the training of the model on the subset of Yelp reviews that are related to expensive businesses, we can review both how many and which documents were assigned to each topic. We can view these topics using the .get_topic_info() method when used on a topic model object. When viewing the returned dataframe, topics are ranked by descending levels of magnitude starting at 0 and ending at the total number of topics. Topic 0 is the most common topic with the greatest number of documents assigned to it, while Topic -1 is a collection of all the documents that do not fit into a topic (outliers) and should not be treated as a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485d377-82ae-4710-bd80-0022a385b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56910634-3a45-4ced-890c-f4bc10c35e92",
   "metadata": {},
   "source": [
    "When running .get_topic_info() on the expensive topic model, we see that Bertopic has returned around 3,500 topics alongside potential outlier topics (labeled as -1). This many topics, however, can be overwhelming to visualize alongside the topics being too specific and complex to be useful. To combat this, we will force Bertopic to further condense the number of topics it generates for the review dataset by using the .reduce_topics(docs, nr_topics = num_topics) method, where the nr_topics argument specifies the number of topics we wish the model to have. After running this method, the current topic model will be replaced by a different topic model that only contains the specified number of topics. For this demonstration, I will reduce the number of topics to 20, although this can be fine-tuned based on the size of the data used.\n",
    "\n",
    "<b>Note:</b> Reducing the number of topics to a value that is too low (i.e. 5 or 10 in my testing) can lead to topics that are too broad, therefore not containing any useful information that is particular to a certain category of businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9de777-7951-46d3-8c92-9f1716e9ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_topic_model.reduce_topics(exp_reviews_to_analyze, nr_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422b7e0-15c9-4c3a-bce2-9943f0de28c2",
   "metadata": {},
   "source": [
    "After replacing the old topic model of expensive businesses with the reduced topic model, we can once again run the .get_topic_info() method to see which topics are deemed to be important by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c062ed-6e08-45f7-a49b-ac31443a86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79195837-cc24-4da1-8205-f90a6a2d7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You can save Bertopic models using .save(\"my_model\", serialization=\"safetensors\")\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "expensive_topic_model.save('expensive_Bert_model', serialization='safetensors', save_ctfidf = True, save_embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb742f-a287-4322-a6ad-fa61701d3388",
   "metadata": {},
   "source": [
    "After reducing the number of topics that the model can produce, the size of each of the significant topics have increased. By reducing the number of topics, we are ensuring that each topic is not too specific or too broad to not be useful, thus providing a better representation of which reviews fit in each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc223b3b-db53-463e-b749-ea2d7d1e7a67",
   "metadata": {},
   "source": [
    "At first glance, the model does not provide much insight as to why each document was assigned to a particular topic. To get a better understand of why, we can first call the ```.approximate_distribution(docs, calculate_tokens = True)``` method to calculate the topic distributions on a token level, then we can pass this calculated distribution into the ```.visualize_approximate_distribution(doc[i], topic_token_distr[i])``` method to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee0fa3-7e18-453f-9a85-e9b576825b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the topic distributions on a token-level\n",
    "exp_topic_distr, exp_topic_token_distr = expensive_topic_model.approximate_distribution(exp_reviews_to_analyze, calculate_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bba85-c4f4-46d7-9899-e142de5ea742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the token-level distributions\n",
    "fig = expensive_topic_model.visualize_approximate_distribution(exp_reviews_to_analyze[0], exp_topic_token_distr[0])\n",
    "fig.to_html('exp_app_dist.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32527a-16cd-42fc-be12-6dfbdca9bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review that the above distribution relates to for reference:\n",
    "exp_reviews_to_analyze[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16f719-7f55-43b4-ad6f-bf73305607aa",
   "metadata": {},
   "source": [
    "There are two things we can gather from the results of this visualization. The first is that certain words have a greater influence on the placement of a review within certain categories than others. For example, the combination \"delicious food\" is a significant indicator that the review is about food and restaurants, whereas the string \"big shout out (to person)\" more loosely implies service was performed. The second thing we can gather is the effect of the bidirectional context that is interpreted by the Bertopic model. For example, the significant words (i.e. \"delicious food\") that point towards a topic always have tails on both the left and right side of the word(s) that decrease in importance the farther the distance from the word. This implies that the model is considering not just how the word fits a review into a certain topic, but how the context around the word also helps to decide which topic a review should be placed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d09aa4-a724-4d01-9f4a-620e7c35a200",
   "metadata": {},
   "source": [
    "To get a better understanding of the relation between reviews of expensive businesses, there are three main methods we can use to visualize their similarities. The first two methods visualize relation on the topic level when the reviews are already grouped, and the third method visualizes reviews on the individual document level, which provides insight as to why each document is grouped into each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047b95a-2cae-4016-abd0-62ec88a6fc4d",
   "metadata": {},
   "source": [
    "The first method, to display the relationship between topics, is to create a visualization that relays how closely related each topic is to the next. We can model this relationship by considering the “distance” between them. The farther the distance between two topics, the more unrelated they are. On the contrary, the shorter the distance, the closer in relation two topics are. As an example, we would expect the statements \"the doctor performs a surgery\" and \"the nurse helps a patient\" to have a relatively close distance because they both convey information about performing tasks within the same human occupation. These two statements would likely be placed within the same topic called \"medical\" and would be located within a circle of influence. However, \"the dog barks\" would have a relatively greater distance between the previous two statements as there is little relation between the two sentences. This statement would be placed within its own topic called \"animals\" and would be some distance away from the \"medical\" topic. We can visualize this distance using the .visualize_topics() method when called on a topic model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f172cf-9427-4c31-bd3c-186e99f6fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = expensive_topic_model.visualize_topics()\n",
    "plot(fig, filename = 'expensive_intertopic_dist_plot.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6534c40-6901-4224-a9d9-e3f7b2c1b719",
   "metadata": {},
   "source": [
    "When looking at the intertropic distance map, we can see that Topic 0 which is based around restaurants and food is much closer to other topics centered around restaurants, such as Topic 13 which is about bars and dinner. On the other hand, the topics related to store service and appointments, such as Topic 1 (shops) and Topic 2 (salons) are a much greater distance away. These clusters within the visualization make it simple to pinpoint which topics are closely related, while the size of each topic’s sphere of influence gives insight into how significant each topic is as reviewers will tend to mention the topics that matter to them with greater frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb85c3-5826-46c7-b164-959316393373",
   "metadata": {},
   "source": [
    "The second method, used to also visualize the relationship between topics, is to create a heatmap that relays the similarities between the topics. For this visualization model, we will be using a heatmap generated based on two topic’s similarity score. The closer this score is to 1, the more related the topics are, while the closer this score is to 0, the more unrelated the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171453b-954a-41f2-b4de-8545924808a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = expensive_topic_model.visualize_heatmap(n_clusters = 5)\n",
    "plot(fig, filename = 'expensive_similarity_mat_plot.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1ad9f-4844-4ac1-b27a-c64a96ac6c3b",
   "metadata": {},
   "source": [
    "From this similarity heatmap, we can see which aspects of a business are conditioned on each other. For example, Topic 0 and Topic 13 have a relatively high similarity score of 0.648. Knowing that Topic 0 relates to restaurants and Topic 13 partially relates to music, it can be inferred that in order for an expensive restaurant to improve their reviews, they must set a fine atmosphere complete with appropriate and enjoyable music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc84fb-bd57-4290-add6-5dad470dceac",
   "metadata": {},
   "source": [
    "The third method to analyze the relation between reviews is done on the individual document level. Similarly to the intertropic distance map we created by using the first method, we can use the concept of “distance” to model the relationships between individual reviews. Because each review is displayed on this graph, we can also visualize the topic each review is a part of by seeing which cluster each review belongs to. Each review is colored to demonstrate which topic cluster each review is a member of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944cc6-a364-43c4-bb13-7d22837c2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = expensive_topic_model.visualize_documents(exp_reviews_to_analyze)\n",
    "plot(fig, filename = 'expensive_document_dist_plot.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11b1b5-a6a7-4436-9e19-3d3ddc59e35a",
   "metadata": {},
   "source": [
    "With this visualization, we can toggle the exact topics we wish to see by clicking on the legend in the top right. When observing the top four topics on the graph, we can see some patterns emerge that coincide with the results from the other two methods. From the intertropical distance model, we expect the documents clustered around Topic 0 to be significantly farther away from the documents that make up the clusters up documents 1, 2, and 3, and 4, and this is the case. Furthermore, based on the similarity matrix, we see a lot of expected overlap between Topics 1 and 4, and Topics 2 and 3, which have high similarity scores between each respective pair of topics. By visualizing the documents on the individual level, we are able to combine the findings of the previous two methods while also being able to visualize the variance that is present between each document and its respective topic. As expected, the larger topics with larger spheres of influence tend to have greater variation with regards to the distance from the center that their associated documents are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c2d6b-8dab-400b-ae8f-887c7c31f5a9",
   "metadata": {},
   "source": [
    "With its immense support for visualizations and powerful tools to assess sentiment and meaning using context, BERTopic has been demonstrably shown to be a powerful tool for language analysis. For businesses that want to improve client reviews, the most important step is the first step, identifying what needs to change. When we ran the BERTopic model on the subset of expensive businesses, we were able to observe not only which topics were frequently mentioned and critiqued, but also which topics were conditioned on each other. The latter here can arguably be more detrimental to increasing average review ratings as the largest topics are often just a collection of smaller topics. The largest topic may be regarding the restaurant quality, but a quality restaurant is only as good as its food, atmosphere, and customer service which are completely different topics. Now that we understand how to find these dependencies, we can further extend the usage of Bertopic to discover what makes a positive review versus a negative one using \"classes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675a9d0-e998-4e21-885a-3de8a0bc3534",
   "metadata": {},
   "source": [
    "### How Can We Use Bertopic to Distinguish Between a Positive and Negative Review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8be5c6-814f-4976-8261-f877c0b2c5a2",
   "metadata": {},
   "source": [
    "Classes within BERTopic provide a way to split the data into categories based on a certain condition or attribute. In this example, I am analyzing medical businesses. Based on the nature of reviews, it is intuitive to split the reviews into classes based on how many stars the review received. For instance, \"Class 1\" would be the collection of all reviews that received a one star rating. If we then modeled these classes using a categorical barchart, we can see the differences between which topics are mentioned more frequently at each star level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f8e4a-02d4-4160-ba96-ff43b99a98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create a subset of the reduced_business dataframe to only include businesses categorized as medical\n",
    "medical = reduced_business[(reduced_business['categories'].str.contains('Doctors')) | (reduced_business['categories'].str.contains('Medical'))]\n",
    "\n",
    "# 2) perform a left merge based on each business' unique ID to only keep medical businesses while also associating each review with their respective business\n",
    "medical_reviews = pd.merge(reviews, medical, how = 'left', on = 'business_id')\n",
    "\n",
    "# 3) to perform any categorical analysis in the future, we will drop the few rows in which category information is missing\n",
    "medical_reviews = medical_reviews.dropna(subset=['categories'])\n",
    "\n",
    "# 4) reset the index of the new dataframe (will be necessary if a class-based analysis is performed)\n",
    "medical_reviews = medical_reviews.reset_index()\n",
    "medical_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb3694-d31b-4010-bdc0-b964a9b3358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_reviews_to_analyze = medical_reviews['text'].tolist()\n",
    "\n",
    "# initiate classes split on the number of stars each review received\n",
    "classes = [star for star in medical_reviews['stars']]\n",
    "medical_topic_model = BERTopic(vectorizer_model = vectorizer_model, umap_model = umap_model,\n",
    "                               hdbscan_model = hdbscan_model, representation_model = representation_model)\n",
    "\n",
    "medical_topics, medical_probs = medical_topic_model.fit_transform(medical_reviews_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac841b77-1ad1-4443-9a0b-6656c5befe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the number of topics for topic clarity\n",
    "medical_topic_model.reduce_topics(medical_reviews_to_analyze, nr_topics = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94908fe5-9ed2-4d7c-bd4d-d935019f0f5f",
   "metadata": {},
   "source": [
    "We can create a new object formed using the .topics_by_class(docs, classes) method that can be passed into a categorical barchart visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227635e-fc37-4f4e-ba53-06beec2e3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this object will be used to model the frequency of topics discussed split by class\n",
    "medical_topics_per_class = medical_topic_model.topics_per_class(medical_reviews_to_analyze, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3e05e-6287-42eb-bb3f-860a8b414919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You can save Bertopic models using .save(\"my_model\", serialization=\"safetensors\")\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "medical_topic_model.save('Medical_Bert_model', serialization='safetensors', save_ctfidf = True, save_embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63080a7-7cf6-45b1-b882-ffbf4854f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a83967-f24d-4557-8653-700a4a4b1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = medical_topic_model.visualize_topics_per_class(medical_topics_per_class)\n",
    "plot(fig, filename = 'medical_by_class_plot.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19025ad-73d3-440e-b1c9-079ef4a2e393",
   "metadata": {},
   "source": [
    "When looking at the visualization, two types of topics begin to stand out, those that have a high frequency at the extremes (i.e. 1 and 5 star reviews) but a low frequency elsewhere, and those that only begin to appear as the reviews approach 5 stars. The first kind of review is best shown by Topic 0 which is represented by words such as appointment and doctor. These types could be considered the \"make or break\" topics for reviews. They are either mentioned because the experience is phenomenal, or because the experience was particularly horrid. In the example of medical businesses, doctors are the central component. If a patient has a memorable doctor, whether that is for a positive or negative reason, it will affect their entire perception of the visit because it was the entire point of the visit most of the time. There is no \"atmosphere\" to make up for a poor doctor like there is in the restaurant industry. Thus, reviews that were elevated to 5 stars or tanked to 1 most likely had a memorable doctor experience that either \"made\" or \"broke\" the trip for the customer. The second kind of topic is the \"nice to have\" variety. These kinds of topics are predominantly noticed when, despite being unnecessary for a positive experience, they make a noticeable impact on the customer experience in addition to their main point of attending. This is best shown by Topic 8 which is based around environment and comfort. Within one star reviews, the environment is hardly noticed because there are other major factors that likely overshadowed it, such as a poor doctor. However, once the customer is enjoying their time, they begin to relax a little and take in their surroundings to notice the things that may be missed, such as the relaxing environments in Topic 8 of medical businesses. This can also work the other direction, notably in customer service. Good customer service can often go overlooked, but poor customer service is often remembered with far greater resentment, as demonstrated by Topic 5 of medical business reviews. However, what is instead we wanted to view how these topics changed over time to see what is the most relevant to the modern reviewer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46372d5-5f65-442c-bc28-68d83534798f",
   "metadata": {},
   "source": [
    "### How Can We Use Bertopic to Model Significant Review Topics Over Time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9beaae-d716-4d9d-8781-f548e4c05930",
   "metadata": {},
   "source": [
    "Reviewers are humans, and humans tend to change their minds about what is important to them in a service frequently. How can we model this change of topic significance over time? As long as the reviews are dated accordingly, BERTopic can be used to visualize how frequently a topic is mentioned over a span of time. To illustrate this concept, we will be taking a look at sports and fitness businesses and see how the topics important to their customers have evolved since reviews were being uploaded to the Yelp platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c6283-b389-4978-9ba0-7e689a56a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create a subset of the reduced_business dataframe to only include businesses categorized as sports and fitness\n",
    "sports = reduced_business[(reduced_business['categories'].str.contains('Sports')) | (reduced_business['categories'].str.contains('Fitness'))]\n",
    "\n",
    "# 2) perform a left merge based on each business' unique ID to only keep medical businesses while also associating each review with their respective business\n",
    "sports_reviews = pd.merge(reviews, sports, how = 'left', on = 'business_id')\n",
    "\n",
    "# 3) to perform any categorical analysis in the future, we will drop the few rows in which category information is missing\n",
    "sports_reviews = sports_reviews.dropna(subset=['categories'])\n",
    "\n",
    "# 4) modify the \"date\" column to include just the year to model frequency of topics over time\n",
    "sports_reviews['date'] = pd.to_datetime(sports_reviews['date']).dt.strftime('%Y')\n",
    "\n",
    "# 5) reset the index of the new dataframe (will be necessary if a class-based analysis is performed)\n",
    "sports_reviews = sports_reviews.reset_index()\n",
    "sports_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a4b94-6ff3-4e4f-8844-6217bc163780",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_reviews_to_analyze = sports_reviews['text'].tolist()\n",
    "sports_dates = sports_reviews['date'].tolist()\n",
    "\n",
    "sports_topic_model = BERTopic(vectorizer_model = vectorizer_model, umap_model = umap_model,\n",
    "                              hdbscan_model = hdbscan_model, representation_model = representation_model)\n",
    "\n",
    "sports_topics, sports_probs = sports_topic_model.fit_transform(sports_reviews_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba95f8-5c66-4c58-81aa-05d698ab8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_topic_model.reduce_topics(sports_reviews_to_analyze, nr_topics = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb840cf-d1fb-4c3d-a284-6f30ac065ab6",
   "metadata": {},
   "source": [
    "We can create a new object formed using the .topics_over_time(docs, dates) method that can be passed into a frequency over time visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d9be0-7e11-4428-b521-9c26d5438392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_topics_over_time = sports_topic_model.topics_over_time(sports_reviews_to_analyze, sports_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b00a4a-b32d-40a8-b717-00badc1afb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913edce8-048b-481e-a322-66c451415d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You can save Bertopic models using .save(\"my_model\", serialization=\"safetensors\")\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sports_topic_model.save('Sports_Bert_model', serialization='safetensors', save_ctfidf = True, save_embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae24cf-f7fb-4fea-b069-b9e765d3004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I removed review data from 2022 for this visualization as there was not enough data points to generate a good plot using it\n",
    "fig = sports_topic_model.visualize_topics_over_time(sports_topics_over_time[sports_topics_over_time['Timestamp'] != '2022-01-01'],\n",
    "                                                    normalize_frequency = True)\n",
    "plot(fig, filename = 'sports_topics_over_time_plot.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba648b-8cb3-413c-8731-4d0b3dff44a6",
   "metadata": {},
   "source": [
    "When looking at the normalized frequency distribution over time, we begin to understand how customers' priorities changed over time. Starting by looking at the elephant in the room, 2020 has a massive frequency spike for one of the topics. This topic, of course, resides over the issue of masking in public areas, undoubtedly applicable sports and fitness businesses. During this time, companies' decisions regarding how they would enforce masking policies were always under intense scrutiny from reviewers. As can be concluded by this chart, having an effective and popular masking policy would most likely lead to better reviews quicker. With this context from recent history, topics of importance can similarly be applied to other topic areas. For example, Topic 7 demonstrates how the demand for quality pool instructors has steadily increased over the years. On the contrary, Topic 3 shows how the interest in shoes tailored for sports use within certain department stores (i.e. a Big-5 type chain) have been on a decline since 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c316926-6e26-48db-a9b8-62ddcb2a767f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc777401-5603-4bda-85f9-19ef67621d95",
   "metadata": {},
   "source": [
    "Businesses are constantly engaged in a mind game with their customers. These corporations have only a finite amount of capital to spend on generating a positive user experience to make up for the negative impact they impose on them, mainly taking their money in order to turn a profit. But with such limited resources to please the customer and also turn a profit, how can businesses decide what improvements to their service they want to focus on? This is the reasoning as to why being able to model human speech through machine learning models is so revolutionary. Models that utilize the immense power of sentence transformers that turn inconsistent language semantics into simple numerical vectors are able to embrace the chaos of language and return simpler results that can be understood easier by the programmers. Through the usage of BERTopic, and similar models, we are able to pinpoint areas of interest and show how they change over time, or whether they are noticed when a user has a positive or negative experience. It allows businesses to learn how they can better allocate their finite resources to achieve the desired result, higher average star ratings with their reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea212633-cf51-435f-ba59-102bfc308ed7",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1217f5-888c-442a-b5e4-1a8c062f0519",
   "metadata": {},
   "source": [
    "<b>Yelp dataset:</b>  \n",
    "- Link: [Yelp Open Dataset](https://www.yelp.com/dataset)  \n",
    "   \n",
    "<b>BERTopic documentation:</b>  \n",
    "- Title: BERTopic: Neural topic modeling with a class-based TF-IDF procedure  \n",
    "- Author: Grootendorst, Maarten  \n",
    "- Year: 2022  \n",
    "- Link: [BERTopic Documentation](https://maartengr.github.io/BERTopic/index.html#citation)  \n",
    "  \n",
    "<b>VADER Documentation:</b>  \n",
    "- Title: VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text  \n",
    "- Authors: Hutto, C.J. and Gilbert, E.E.  \n",
    "- Year: 2014  \n",
    "- Link: [VADER Documentation](https://github.com/cjhutto/vaderSentiment)\n",
    "\n",
    "<b>Sentence Transformer documentation:</b>  \n",
    "- Title: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks  \n",
    "- Authors: Reimers, Nils and Gurevych, Iryna  \n",
    "- Year: 2019  \n",
    "- Link: [Sentence Transformer Documentation](https://github.com/UKPLab/sentence-transformers/blob/master/index.rst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
